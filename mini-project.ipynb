{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport gc\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train_data = pd.read_parquet('/kaggle/input/amex-parquet/train_data.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After feature engineering, requied preprocessing done directly on dataset","metadata":{}},{"cell_type":"markdown","source":"Categorical data to separate columns, but only some of the categories are dropped","metadata":{}},{"cell_type":"code","source":"temp = pd.get_dummies(df_train_data[\"D_64\"], prefix='D_64')\nfor i in ['O', 'R', 'U']:\n    df_train_data[\"D_64\"+i] = temp[\"D_64_\"+i]\ntemp = pd.get_dummies(df_train_data[\"D_63\"], prefix='D_63')\nfor i in ['CL', 'CO', 'CR']:\n    df_train_data[\"D_63\"+i] = temp[\"D_63_\"+i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"drop identified columns during feature engineering","metadata":{}},{"cell_type":"code","source":"drop_cols = ['R_26', 'D_132', 'D_134', 'D_141', 'R_9', 'D_75', 'D_119', 'B_14', 'D_104', \n             'D_110', 'D_108', 'S_24', 'D_49', 'B_39', 'B_42', 'D_77', 'S_7', 'B_15', \n             'D_42', 'B_33', 'S_22', 'D_58', 'D_87', 'B_23', 'B_7', 'D_118', 'S_2', \n             'D_137', 'D_111', 'D_106', 'B_37', 'B_11', 'D_66', 'D_76', 'B_1', 'B_29', \n             'D_138', 'D_64', 'D_143', 'D_136', 'D_139', 'B_2', 'customer_ID', 'D_74', \n             'D_142', 'D_135', 'D_62', 'D_63', 'D_73', 'S_3', 'D_88', 'D_103']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data = df_train_data.drop(drop_cols,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"fill missing values with median","metadata":{}},{"cell_type":"code","source":"for column in df_train_data.columns:\n    median = df_train_data[column].median()\n    df_train_data[column] = df_train_data[column].fillna(median)\ndf_train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='section1'></a>\n# Feature Engineering\n","metadata":{}},{"cell_type":"markdown","source":"# Categorical data\n1. only columns which does not have numerical values are considered.\n2. unique values are checked and graphed, graphed with target as well\n3. values with low count and original columns are dropped","metadata":{}},{"cell_type":"code","source":"num_cols = dgf_train_data._get_numeric_data().columns\ncate_cols = list(set(df_train_data.columns) - set(num_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cate_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data['D_64'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data.groupby('D_64')['customer_ID'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no need\n# S_2 column split to year, month,day\n# impact of year, month, day was unnoticable, were not added to dataset\ndate = pd.to_datetime(df_train_data['S_2'])\n\ndf_train_data['year'] = date.dt.year\ndf_train_data['month'] = date.dt.month\ndf_train_data['day'] = date.dt.day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\nfor i, k in enumerate([\"D_63\",\"D_64\",\"S_2\"]):\n    plt.subplot(6, 2, i+1)\n    temp_val = pd.DataFrame(df_train_data[k].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp_val.index.name = 'value'\n    temp_val.reset_index(inplace=True)\n    plt.bar(temp_val.index, temp_val['count'], alpha=0.5)\n    plt.xlabel(k)\n    plt.ylabel('frequency')\n    plt.xticks(temp_val.index, temp_val.value)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\nfor i, f in enumerate([\"D_63\",\"D_64\", \"year\",\"month\",\"day\"]):\n    plt.subplot(6, 2, i+1)\n    temp = pd.DataFrame(df_train_data[f][df_train_data.target == 0].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp.index.name = 'value'\n    temp.reset_index(inplace=True)\n    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=0')\n    temp = pd.DataFrame(df_train_data[f][df_train_data.target == 1].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n    temp.index.name = 'value'\n    temp.reset_index(inplace=True)\n    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=1')\n    plt.xlabel(f)\n    plt.ylabel('frequency')\n    plt.legend()\n    plt.xticks(temp.index, temp.value)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.get_dummies(df_train_data[\"D_64\"], prefix='D_64')\nfor i in ['O', 'R', 'U']:\n    df_train_data[\"D_64\"+i] = temp[\"D_64_\"+i]\ntemp = pd.get_dummies(df_train_data[\"D_63\"], prefix='D_63')\nfor i in ['CL', 'CO', 'CR']:\n    df_train_data[\"D_63\"+i] = temp[\"D_63_\"+i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data = df_train_data.drop(cate_cols, axis=1)\ndel temp\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling missing values\n1. Columns with more than 80% missing values are dropped\n2. Other missing values are filled with median","metadata":{}},{"cell_type":"code","source":"missing_data = pd.DataFrame(df_train_data.isnull().sum()/len(df_train_data))\nneed_drop = missing_data.loc[missing_data[0] >= 0.8]\nprint('number of column w/ >= 80% missing value = ', len(need_drop))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_need_drop = list(need_drop.T.columns)\ndf_train_data = df_train_data.drop(cols_need_drop, axis=1)\ndel need_drop,missing_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in df_train_data.columns:\n    median = df_train_data[column].median()\n    df_train_data[column] = df_train_data[column].fillna(median)\ndf_train_data.head()\ndel median\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class imbalance\n1. Target values with 0, resampled to the count of target with 1 value.\n2. Did not applied to some of the models\n3. Helped with kaggle RAM exceeding issue","metadata":{}},{"cell_type":"code","source":"draw_chart = pd.DataFrame(df_train_data['target'].value_counts()).T\nprint('percentage of target value 0 / 1 = ', draw_chart[1]/draw_chart[0], '\\n\\n\\n');\ndraw_chart.plot.barh(align='edge', width=0.5);\ndel draw_chart\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_0 = df_train_data[df_train_data['target']==0]\ntarget_1 = df_train_data[df_train_data['target']==1]\ndel df_train_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_0.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample = target_0.sample(n=1377869, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data = pd.concat([df_sample, target_1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del target_0,target_1,df_sample\ngc.collect()\ndf_train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tried do PCA, was not possible due to not availability of enough memory**","metadata":{}},{"cell_type":"code","source":"# cannot run. not enough memory available\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(df_train_data.drop(['target'],axis=1))\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(df_train_data.shape[1]-1)]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\n#X_pca.head()\ndf = pd.DataFrame({'var':pca.explained_variance_ratio_,\n             'PC':component_names})\nsns.barplot(x='PC',y=\"var\", \n           data=df, color=\"c\");\ndf[df['var']>0.15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Columns with more than 90% correlation are dropped**","metadata":{}},{"cell_type":"code","source":"correlation_mat = df_train_data.corr()\ncorrelation_mat[\"target\"]\ncorrelated_columns = correlation_mat.where(abs(correlation_mat) > 0.9).stack().index.tolist()\n\n# remove correlated columns >90%\ndrop_correlated_columns = []\nfor col1, col2 in correlated_columns:\n    if col1 != col2:\n        drop_correlated_columns.append(col2)\ndrop_correlated_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data = df_train_data.drop(drop_correlated_columns, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tried removing columns with less correlation with target**\n\n**Only for some models**","metadata":{}},{"cell_type":"code","source":"#no need to run, relationship might not be linear\n# drop columns with less correlation with target\ncols_corr_drop = correlation_mat[(correlation_mat[\"target\"]<0.05) & (correlation_mat[\"target\"]>-0.05)].T.columns\nfor col in cols_corr_drop:\n    if col in df_train_data.columns:\n        df_train_data = df_train_data.drop(col, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del correlation_mat,correlated_columns\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all dropped columns are saved in a list, posiible to drop without running above cells again\ndrop_cols = cate_cols\ndrop_cols.extend(cols_need_drop)\ndrop_cols.extend(drop_correlated_columns)\ndrop_cols = list(set(drop_cols))\nlen(drop_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(drop_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Separating target from features and**\n\n**MIN MAX scaling features**","metadata":{}},{"cell_type":"code","source":"X = df_train_data.drop('target',axis=1)\ny = df_train_data['target']\ndel df_train_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Save the scaler to a file\nwith open('scaler.pickle', 'wb') as f:\n    pickle.dump(scaler, f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting data train = 80%, test = 20%**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nx_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\ndel X,y\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n\n    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        df = (pd.concat([y_true, y_pred], axis='columns')\n              .sort_values('prediction', ascending=False))\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        four_pct_cutoff = int(0.04 * df['weight'].sum())\n        df['weight_cumsum'] = df['weight'].cumsum()\n        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n        \n    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        df = (pd.concat([y_true, y_pred], axis='columns')\n              .sort_values('prediction', ascending=False))\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n        total_pos = (df['target'] * df['weight']).sum()\n        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n        df['lorentz'] = df['cum_pos_found'] / total_pos\n        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n        return df['gini'].sum()\n\n    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n\n    g = normalized_weighted_gini(y_true, y_pred)\n    d = top_four_percent_captured(y_true, y_pred)\n\n    return 0.5 * (g + d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"**Methods tried**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\n\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GaussianNB**","metadata":{}},{"cell_type":"code","source":"classifier = GaussianNB()\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training accuracy: {classifier.score(x_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred  =  classifier.predict(x_test)\nprint(amex_metric(pd.DataFrame(y_test,columns=['target']), pd.DataFrame(y_pred,columns=[\"prediction\"]))) \nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to a file\nwith open('/kaggle/working/gaussianNB.pkl', 'wb') as f:\n  pickle.dump(classifier, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del classifier\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SVM**","metadata":{}},{"cell_type":"code","source":"SVM = svm.SVC(kernel='linear',C=1.3,degree=8,cache_size=300) \nSVM.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=SVM.predict(x_test)\nprint(amex_metric(pd.DataFrame(y_test,columns=['target']), pd.DataFrame(y_pred,columns=[\"prediction\"]))) \nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to a file\nwith open('/kaggle/working/gaussianNB.pkl', 'wb') as f:\n  pickle.dump(classifier, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**KNN**","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=20,n_jobs=-1)\nmodel.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model.predict(test_X)\nprint(amex_metric(pd.DataFrame(y_test,columns=['target']), pd.DataFrame(y_pred,columns=[\"prediction\"]))) # 0.572773\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to a file\nwith open('/kaggle/working/knn.pkl', 'wb') as f:\n  pickle.dump(model, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic regrassion**","metadata":{}},{"cell_type":"code","source":"logistic_reg = LogisticRegression()\nlogistic_reg.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=logistic_reg.predict(x_test)\nprint(amex_metric(pd.DataFrame(y_test,columns=['target']), pd.DataFrame(y_pred,columns=[\"prediction\"])))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to a file\nwith open('/kaggle/working/logistic_reg.pkl', 'wb') as f:\n  pickle.dump(logistic_reg, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del logistic_reg\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost**","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(\n            learning_rate=0.02,\n            n_estimators=20,\n            objective=\"binary:logistic\",\n            nthread=4\n        )\nxgb.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred  =  xgb.predict(x_test)\nprint(amex_metric(pd.DataFrame(y_test,columns=['target']), pd.DataFrame(y_pred,columns=[\"prediction\"]))) # 0.572773\nprint(f'Accuracy: {xgb.score(x_test, y_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/xgb_model2.pkl', 'wb') as f:\n  pickle.dump(xgb, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LightGBM**","metadata":{}},{"cell_type":"code","source":"d_train = lgb.Dataset(x_train, label=y_train)\nparams = {'objective': 'binary','metric': 'binary_logloss','boosting': 'gbdt','num_leaves': 100,'reg_lambda' : 60,'colsample_bytree': 0.2,'learning_rate': 0.02,'min_child_samples': 2400,'max_bins': 600,'seed': 42,'verbose': -1}\nlgb_model = lgb.train(params, d_train, 300)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred  =  lgb_model.predict(x_test)\ny_pred[y_pred>=0.5] = 1\ny_pred[y_pred <0.5] = 0\nprint(amex_metric(pd.DataFrame(y_test,columns=['target']), pd.DataFrame(y_pred,columns=[\"prediction\"]))) # 0.572773\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/lgb_model2.pkl', 'wb') as f:\n  pickle.dump(lgb_model, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del lgb_model #x_train,x_test,y_train,y_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x_train,x_test,y_train,y_test\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = pd.read_parquet('/kaggle/input/amex-parquet/test_data.parquet')\ntest_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = test_dataset.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding created columns**","metadata":{}},{"cell_type":"code","source":"temp = pd.get_dummies(test_dataset[\"D_64\"], prefix='D_64')\nfor i in ['O', 'R', 'U']:\n    test_dataset[\"D_64\"+i] = temp[\"D_64_\"+i]\ntemp = pd.get_dummies(test_dataset[\"D_63\"], prefix='D_63')\nfor i in ['CL', 'CO', 'CR']:\n    test_dataset[\"D_63\"+i] = temp[\"D_63_\"+i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop columns**","metadata":{}},{"cell_type":"code","source":"test_dataset = test_dataset.drop(col,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill missing with median**","metadata":{}},{"cell_type":"code","source":"for column in test_dataset.columns:\n    if column=='customer_ID': continue\n    median = test_dataset[column].median()\n    test_dataset[column] = test_dataset[column].fillna(median)\ntest_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load same scaler function and scale test dataset**","metadata":{}},{"cell_type":"code","source":"with open('scaler.pickle', 'rb') as f:\n    scaler = pickle.load(f)\ntest_X = scaler.fit_transform(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load saved model**","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/xgb_model2.pkl', 'rb') as f:\n  model = pickle.load(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred  =  model.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only for lightgbm\ny_pred[y_pred>=0.5] = 1\ny_pred[y_pred <0.5] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_dataset = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'customer_ID': sample_dataset.customer_ID, 'prediction': y_pred.astype(int)})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save submissions as a csv**","metadata":{}},{"cell_type":"code","source":"output.to_csv('submission_lgb5.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}